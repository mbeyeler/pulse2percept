# Pulse to percept

Models for retinal prosthetics

## Project charter

### Objectives

By 2020 roughly 200 million people will suffer from retinal diseases such as
macular degeneration or retinitis pigmentosa. Consequently, a variety of retinal
sight restoration procedures are being developed to target these diseases.
Electronic prostheses (currently being implanted in patients) directly stimulate
remaining retinal cells using electrical current, analogous to a cochlear
implant. Optogenetic prostheses (soon to be implanted in human) use optogenetic
proteins to make remaining retinal cells responsive to light, then use light
diodes (natural illumination is inadequate) implanted in the eye to stimulate
these light sensitive cells.

All these technologies require a coding method to translate the visual input
into a light or electrical stimulation protocol. Our goal is to develop an
optimized stimulation protocol that can be tested on the Second Sight Medical
Products Argus 2 prosthetic. Our hope is that eventually this will provide an
improved stimulation protocol for these and other devices (both electronic and
optogenetics).


Current devices essentially use a linear coding scheme that maps image luminance
directly to stimulation amplitude/rate. However such linear codes have been
shown to be deeply inadequate.

What's new in your approach and why do you think it will be successful? Our plan
is to develop and test a neurophysiologically inspired algorithm for improved
stimulation protocols in patients implanted with the 2nd Sight electronic
prosthesis. Our algorithm is novel in that will rely on  an existing
quantitative and validated model (developed by Project Leads Fine and Boynton)
of the perceptual effects of retinal stimulation. The current model can predict
the patients percept generated by any pattern of electrical stimulation. We will
'reverse-engineer' this model so we are capable of estimating the optimized
pulse pattern that will generate the ‘best possible’ visual percept.


Any improvement in the perceptual outcomes of these devices will be of huge
importance, given that current perceptual outcomes in patients implanted with
prosthetics are surprisingly poor. The company 2nd sight and other companies are
extremely interested in the outcome.



Ideally an inverse model could be derived by simply inverting the transfer function 
(input-output function) of each part of the model. However, the
transfer function of some boxes will not be easily inverted because they
non-linearly depend on the entire history of the signals passed through them.
In the future, we hope to apply two distinct approaches. The first is
to model these blocks as dynamical systems that have 'state' (e.g., the
integral of the current passed through them) and derive or approximate the
model inverse using these states. The second approach would be to employ
machine learning to learn the desired input-output relationship for each box,
which has the advantage of deferring most of the computational load to the
training phase, allowing for rapid classification/regression in the testing
phase. 

In order  to create the amount of training data needed to achieve good
generalization performance we will use the existing forward model of the
perceptual effects of electrical stimulation over time to generate fake
patients', and use these 'fake' responses to derive a loss function for the
machine learning algorithm.


##INSTALLATION

###Prerequisites
pulse2percept requires the following software installed for your platform:

1. [Python](http://www.python.org) 2.7 or >= 3.4

2. [NumPy](http://www.numpy.org)

3. [SciPy](http://www.scipy.org)

4. [JobLib](https://github.com/joblib/joblib)

5. [Dask](https://github.com/dask/dask)


### Development version

Use the command:

```
$ git clone https://github.com/uwescience/pulse2percept.git
$ cd pulse2percept
$ python setup.py install
```

To test pulse2percept after installation, execute in Python:
```
>>> import pulse2percept
```


For Windows users you will need to install the ffmpeg codec. A good description 
of how to do that is here
http://adaptivesamples.com/how-to-install-ffmpeg-on-windows/


#Descriptions of functions

##electrode2currentmap

Defines conversion parameters (micron2deg, deg2micron)

Creates an object, Electrode that includes:
        x and y position
        a method (current_map) that creates a current map

Creates an object, ElectrodeGrid
        consists of a list of electrodes
        creates a current map by summing across all the electrodes
    

Defines an object, Retina that includes:
    the retinal grid in microns
    loads or creates the oyster axon map

    the object contains a method (cm2ecm) that takes the current map on the retina and turns it into an effective current
        map by passing it through the axonal look up table
    

# oyster

Defines a list of axon pathway (jansonius)

Defines a mapping of how each pixel in the retina space is affected by the 
underlying ganglion cell axons (makeAxonMap).

#effectivecurrent2brightness

Defines gamma function (gamma)
Creates an object, TemporalModel that contains:
    a list of parameters
    Three methods, fast_response, charge_accumulation and slow_response that 
represent the three stages of the Alan model
    Returns brightness

